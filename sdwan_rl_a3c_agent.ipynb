{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code credit - partially based on https://github.com/tensorflow/models/blob/master/research/a3c_blogpost/a3c_cartpole.py\n",
    "import os\n",
    "import threading\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import gym_sdwan\n",
    "import numpy as np\n",
    "import keras\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import csv\n",
    "\n",
    "import logging \n",
    "mpl_logger = logging.getLogger('matplotlib') \n",
    "mpl_logger.setLevel(logging.WARNING) \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# import threading\n",
    "# import gym\n",
    "# import multiprocessing\n",
    "# import numpy as np\n",
    "# from queue import Queue\n",
    "# import argparse\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python import keras\n",
    "# from tensorflow.python.keras import layers\n",
    "\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Run A3C algorithm on the game '\n",
    "#                                              'Cartpole.')\n",
    "# parser.add_argument('--algorithm', default='a3c', type=str,\n",
    "#                     help='Choose between \\'a3c\\' and \\'random\\'.')\n",
    "# parser.add_argument('--train', dest='train', action='store_true',\n",
    "#                     help='Train our model.')\n",
    "# parser.add_argument('--lr', default=0.001,\n",
    "#                     help='Learning rate for the shared optimizer.')\n",
    "# parser.add_argument('--update-freq', default=20, type=int,\n",
    "#                     help='How often to update the global model.')\n",
    "# parser.add_argument('--max-eps', default=1000, type=int,\n",
    "#                     help='Global maximum number of episodes to run.')\n",
    "# parser.add_argument('--gamma', default=0.99,\n",
    "#                     help='Discount factor of rewards.')\n",
    "# parser.add_argument('--save-dir', default='/tmp/', type=str,\n",
    "#                     help='Directory in which you desire to save the model.')\n",
    "# args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticModel(keras.Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(ActorCriticModel, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.dense1 = layers.Dense(100, activation='relu')\n",
    "        self.policy_logits = layers.Dense(action_size)\n",
    "        self.dense2 = layers.Dense(100, activation='relu')\n",
    "        self.values = layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        x = self.dense1(inputs)\n",
    "        logits = self.policy_logits(x)\n",
    "        v1 = self.dense2(inputs)\n",
    "        values = self.values(v1)\n",
    "        return logits, values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record(episode,\n",
    "           episode_reward,\n",
    "           worker_idx,\n",
    "           global_ep_reward,\n",
    "           result_queue,\n",
    "           total_loss,\n",
    "           num_steps):\n",
    "    \"\"\"Helper function to store score and print statistics.\n",
    "\n",
    "        Arguments:\n",
    "        episode: Current episode\n",
    "        episode_reward: Reward accumulated over the current episode\n",
    "        worker_idx: Which thread (worker)\n",
    "        global_ep_reward: The moving average of the global reward\n",
    "        result_queue: Queue storing the moving average of the scores\n",
    "        total_loss: The total loss accumualted over the current episode\n",
    "        num_steps: The number of steps the episode took to complete\n",
    "    \"\"\"\n",
    "    if global_ep_reward == 0:\n",
    "        global_ep_reward = episode_reward\n",
    "    else:\n",
    "        global_ep_reward = global_ep_reward * 0.99 + episode_reward * 0.01\n",
    "    print(\n",
    "        f\"Episode: {episode} | \"\n",
    "        f\"Moving Average Reward: {int(global_ep_reward)} | \"\n",
    "        f\"Episode Reward: {int(episode_reward)} | \"\n",
    "        f\"Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | \"\n",
    "        f\"Steps: {num_steps} | \"\n",
    "        f\"Worker: {worker_idx}\"\n",
    "    )\n",
    "    result_queue.put(global_ep_reward)\n",
    "    return global_ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class RandomAgent:\n",
    "#     \"\"\"Random Agent that will play the specified game\n",
    "\n",
    "#     Arguments:\n",
    "#       env_name: Name of the environment to be played\n",
    "#       max_eps: Maximum number of episodes to run agent for.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, env_name, max_eps):\n",
    "#         self.env = gym.make(env_name)\n",
    "#         self.max_episodes = max_eps\n",
    "#         self.global_moving_average_reward = 0\n",
    "#         self.res_queue = Queue()\n",
    "\n",
    "#     def run(self):\n",
    "#         reward_avg = 0\n",
    "#     for episode in range(self.max_episodes):\n",
    "#         done = False\n",
    "#         self.env.reset()\n",
    "#         reward_sum = 0.0\n",
    "#         steps = 0\n",
    "#         while not done:\n",
    "#             # Sample randomly from the action space and step\n",
    "#             _, reward, done, _ = self.env.step(self.env.action_space.sample())\n",
    "#             steps += 1\n",
    "#             reward_sum += reward\n",
    "#         # Record statistics\n",
    "#         self.global_moving_average_reward = record(episode,\n",
    "#                                                    reward_sum,\n",
    "#                                                    0,\n",
    "#                                                    self.global_moving_average_reward,\n",
    "#                                                    self.res_queue, 0, steps)\n",
    "\n",
    "#         reward_avg += reward_sum\n",
    "#     final_avg = reward_avg / float(self.max_episodes)\n",
    "#     print(\"Average score across {} episodes: {}\".format(self.max_episodes, final_avg))\n",
    "#     return final_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MasterAgent():\n",
    "    def __init__(self):\n",
    "        self.game_name = \"Sdwan-stat-v0\"\n",
    "#         save_dir = args.save_dir\n",
    "#         self.save_dir = save_dir\n",
    "#         if not os.path.exists(save_dir):\n",
    "#             os.makedirs(save_dir)\n",
    "\n",
    "        env = gym.make(self.game_name)\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.opt = tf.train.AdamOptimizer(args.lr, use_locking=True)\n",
    "        print(self.state_size, self.action_size)\n",
    "\n",
    "        self.global_model = ActorCriticModel(self.state_size, self.action_size)  # global network\n",
    "        self.global_model(tf.convert_to_tensor(np.random.random((1, self.state_size)), dtype=tf.float32))\n",
    "\n",
    "    def train(self):\n",
    "        if args.algorithm == 'random':\n",
    "            random_agent = RandomAgent(self.game_name, args.max_eps)\n",
    "            random_agent.run()\n",
    "            return\n",
    "\n",
    "        res_queue = Queue()\n",
    "\n",
    "        workers = [Worker(self.state_size,\n",
    "                          self.action_size,\n",
    "                          self.global_model,\n",
    "                          self.opt, res_queue,\n",
    "                          i, game_name=self.game_name,\n",
    "                          save_dir=self.save_dir) for i in range(multiprocessing.cpu_count())]\n",
    "\n",
    "        for i, worker in enumerate(workers):\n",
    "            print(\"Starting worker {}\".format(i))\n",
    "            worker.start()\n",
    "\n",
    "        moving_average_rewards = []  # record episode reward to plot\n",
    "        while True:\n",
    "            reward = res_queue.get()\n",
    "            if reward is not None:\n",
    "                moving_average_rewards.append(reward)\n",
    "            else:\n",
    "                break\n",
    "        [w.join() for w in workers]\n",
    "\n",
    "        plt.plot(moving_average_rewards)\n",
    "        plt.ylabel('Moving average ep reward')\n",
    "        plt.xlabel('Step')\n",
    "        plt.savefig(os.path.join(self.save_dir,\n",
    "                                 '{} Moving Average.png'.format(self.game_name)))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def play(self):\n",
    "        env = gym.make(self.game_name).unwrapped\n",
    "        state = env.reset()\n",
    "        model = self.global_model\n",
    "        model_path = os.path.join(self.save_dir, 'model_{}.h5'.format(self.game_name))\n",
    "        print('Loading model from: {}'.format(model_path))\n",
    "        model.load_weights(model_path)\n",
    "        done = False\n",
    "        step_counter = 0\n",
    "        reward_sum = 0\n",
    "\n",
    "        try:\n",
    "            while not done:\n",
    "                env.render(mode='rgb_array')\n",
    "                policy, value = model(tf.convert_to_tensor(state[None, :], dtype=tf.float32))\n",
    "                policy = tf.nn.softmax(policy)\n",
    "                action = np.argmax(policy)\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                reward_sum += reward\n",
    "                print(\"{}. Reward: {}, action: {}\".format(step_counter, reward_sum, action))\n",
    "                step_counter += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Received Keyboard Interrupt. Shutting down.\")\n",
    "        finally:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def store(self, state, action, reward):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(threading.Thread):\n",
    "    # Set up global variables across different threads\n",
    "    global_episode = 0\n",
    "    # Moving average reward\n",
    "    global_moving_average_reward = 0\n",
    "    best_score = 0\n",
    "    save_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_size,\n",
    "                 action_size,\n",
    "                 global_model,\n",
    "                 opt,\n",
    "                 result_queue,\n",
    "                 idx,\n",
    "                 game_name='CartPole-v0',\n",
    "                 save_dir='/tmp'):\n",
    "        super(Worker, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.result_queue = result_queue\n",
    "        self.global_model = global_model\n",
    "        self.opt = opt\n",
    "        self.local_model = ActorCriticModel(self.state_size, self.action_size)\n",
    "        self.worker_idx = idx\n",
    "        self.game_name = game_name\n",
    "        self.env = gym.make(self.game_name).unwrapped\n",
    "        self.save_dir = save_dir\n",
    "        self.ep_loss = 0.0\n",
    "\n",
    "    def run(self):\n",
    "        total_step = 1\n",
    "        mem = Memory()\n",
    "        while Worker.global_episode < args.max_eps:\n",
    "            current_state = self.env.reset()\n",
    "            mem.clear()\n",
    "            ep_reward = 0.\n",
    "            ep_steps = 0\n",
    "            self.ep_loss = 0\n",
    "\n",
    "            time_count = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                logits, _ = self.local_model(\n",
    "                    tf.convert_to_tensor(current_state[None, :],\n",
    "                                         dtype=tf.float32))\n",
    "                probs = tf.nn.softmax(logits)\n",
    "\n",
    "                action = np.random.choice(self.action_size, p=probs.numpy()[0])\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                if done:\n",
    "                    reward = -1\n",
    "                ep_reward += reward\n",
    "                mem.store(current_state, action, reward)\n",
    "\n",
    "                if time_count == args.update_freq or done:\n",
    "                    # Calculate gradient wrt to local model. We do so by tracking the\n",
    "                    # variables involved in computing the loss by using tf.GradientTape\n",
    "                    with tf.GradientTape() as tape:\n",
    "                        total_loss = self.compute_loss(done,\n",
    "                                                       new_state,\n",
    "                                                       mem,\n",
    "                                                       args.gamma)\n",
    "                    self.ep_loss += total_loss\n",
    "                    # Calculate local gradients\n",
    "                    grads = tape.gradient(total_loss, self.local_model.trainable_weights)\n",
    "                    # Push local gradients to global model\n",
    "                    self.opt.apply_gradients(zip(grads,\n",
    "                                               self.global_model.trainable_weights))\n",
    "                    # Update local model with new weights\n",
    "                    self.local_model.set_weights(self.global_model.get_weights())\n",
    "\n",
    "                    mem.clear()\n",
    "                    time_count = 0\n",
    "\n",
    "                    if done:  # done and print information\n",
    "                        Worker.global_moving_average_reward = \\\n",
    "                            record(Worker.global_episode, ep_reward, self.worker_idx,\n",
    "                                   Worker.global_moving_average_reward, self.result_queue,\n",
    "                                   self.ep_loss, ep_steps)\n",
    "                        # We must use a lock to save our model and to print to prevent data races.\n",
    "                        if ep_reward > Worker.best_score:\n",
    "                            with Worker.save_lock:\n",
    "                                print(\"Saving best model to {}, episode score: {}\".\n",
    "                                      format(self.save_dir, ep_reward))\n",
    "                                self.global_model.save_weights(\n",
    "                                    os.path.join(self.save_dir,\n",
    "                                                 'model_{}.h5'.format(self.game_name))\n",
    "                                )\n",
    "                                Worker.best_score = ep_reward\n",
    "                    Worker.global_episode += 1\n",
    "                ep_steps += 1\n",
    "\n",
    "                time_count += 1\n",
    "                current_state = new_state\n",
    "                total_step += 1\n",
    "        self.result_queue.put(None)\n",
    "\n",
    "    def compute_loss(self,\n",
    "                     done,\n",
    "                     new_state,\n",
    "                     memory,\n",
    "                     gamma=0.99):\n",
    "        if done:\n",
    "            reward_sum = 0.  # terminal\n",
    "        else:\n",
    "            reward_sum = self.local_model(\n",
    "                tf.convert_to_tensor(new_state[None, :],\n",
    "                                     dtype=tf.float32))[-1].numpy()[0]\n",
    "\n",
    "        # Get discounted rewards\n",
    "        discounted_rewards = []\n",
    "        for reward in memory.rewards[::-1]:  # reverse buffer r\n",
    "            reward_sum = reward + gamma * reward_sum\n",
    "            discounted_rewards.append(reward_sum)\n",
    "        discounted_rewards.reverse()\n",
    "\n",
    "        logits, values = self.local_model(\n",
    "            tf.convert_to_tensor(np.vstack(memory.states),\n",
    "                                 dtype=tf.float32))\n",
    "        # Get our advantages\n",
    "        advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],\n",
    "                                dtype=tf.float32) - values\n",
    "        # Value loss\n",
    "        value_loss = advantage ** 2\n",
    "\n",
    "        # Calculate our policy loss\n",
    "        policy = tf.nn.softmax(logits)\n",
    "        entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=policy, logits=logits)\n",
    "\n",
    "        policy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=memory.actions,\n",
    "                                                                     logits=logits)\n",
    "        policy_loss *= tf.stop_gradient(advantage)\n",
    "        policy_loss -= 0.01 * entropy\n",
    "        total_loss = tf.reduce_mean((0.5 * value_loss + policy_loss))\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: Sdwan-stat-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Sdwan-stat-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-2fb0f6b52737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMasterAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmaster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-e3169ace457a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#             os.makedirs(save_dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgame_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m: No registered env with id: Sdwan-stat-v0"
     ]
    }
   ],
   "source": [
    "master = MasterAgent()\n",
    "master.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser('DQN Agent')\n",
    "#     parser.add_argument(\n",
    "#         '--n-episodes',\n",
    "#         type=int,\n",
    "#         default=100)\n",
    "#     parser.add_argument(\n",
    "#         '--n-max-ticks',\n",
    "#         type=int,\n",
    "#         default=300)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     print(args)\n",
    "#     master = MasterAgent()\n",
    "#     if args.train:\n",
    "#         master.train()\n",
    "#     else:\n",
    "#         master.play()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
